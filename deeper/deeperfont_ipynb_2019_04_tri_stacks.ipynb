{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deeperfont.ipynb 2019-04 tri_stacks",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/terryspitz/ipython_notebooks/blob/master/deeper/deeperfont_ipynb_2019_04_tri_stacks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjX8iC8oEXWI",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "keoxQa2SSnpU",
        "outputId": "3c045fff-a1df-4bfe-c596-0a1bba6a57e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1291
        }
      },
      "source": [
        "# Install dependencies for colab.research.google.com.\n",
        "#!pip install -I tensorflow==1.11.0\n",
        "#!pip install tensorflow-probability-gpu\n",
        "#!pip install tfp-nightly\n",
        "\n",
        "# Install dependencies for GCP Deep Learning VM.\n",
        "! pip3 install tensorflow-gpu\n",
        "! pip3 install tensorflow-probability-gpu\n",
        "! pip3 install wrapt\n",
        "\n",
        "# Install dependencies locally\n",
        "! pip3 install tensorflow\n",
        "! pip3 install tensorflow-probability\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu in c:\\users\\terry\\anaconda3\\lib\\site-packages (1.13.1)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.13.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.0.7)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (3.7.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.7.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.16.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.0.9)\n",
            "Requirement already satisfied: gast>=0.2.0 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.11.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.30.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.6.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu) (2.6.11)\n",
            "Requirement already satisfied: h5py in c:\\users\\terry\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow-gpu) (2.7.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\terry\\anaconda3\\lib\\site-packages (from protobuf>=3.6.1->tensorflow-gpu) (39.0.1)\n",
            "Requirement already satisfied: mock>=2.0.0 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu) (2.0.0)\n",
            "Requirement already satisfied: pbr>=0.11 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu) (5.1.3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "You are using pip version 19.0.3, however version 19.1 is available.\n",
            "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-probability-gpu in c:\\users\\terry\\anaconda3\\lib\\site-packages (0.4.0)\n",
            "Requirement already satisfied: tensorflow-gpu>=1.10.0 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-probability-gpu) (1.13.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-probability-gpu) (1.16.2)\n",
            "Requirement already satisfied: six>=1.10.0 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-probability-gpu) (1.11.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (0.30.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (3.7.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (1.0.7)\n",
            "Requirement already satisfied: astor>=0.6.0 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (0.6.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (1.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (1.0.9)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (1.13.0)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (1.13.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (0.7.1)\n",
            "Requirement already satisfied: setuptools in c:\\users\\terry\\anaconda3\\lib\\site-packages (from protobuf>=3.6.1->tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (39.0.1)\n",
            "Requirement already satisfied: h5py in c:\\users\\terry\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (2.7.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (2.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (2.6.11)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (0.14.1)\n",
            "Requirement already satisfied: pbr>=0.11 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (5.1.3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "You are using pip version 19.0.3, however version 19.1 is available.\n",
            "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wrapt in c:\\users\\terry\\anaconda3\\lib\\site-packages (1.10.11)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "You are using pip version 19.0.3, however version 19.1 is available.\n",
            "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in c:\\users\\terry\\anaconda3\\lib\\site-packages (1.13.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.1)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow) (1.13.1)\n",
            "Requirement already satisfied: six>=1.10.0 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow) (1.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow) (1.13.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.9)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.7)\n",
            "Requirement already satisfied: astor>=0.6.0 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow) (0.6.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow) (0.30.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.2)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow) (0.7.1)\n",
            "Requirement already satisfied: setuptools in c:\\users\\terry\\anaconda3\\lib\\site-packages (from protobuf>=3.6.1->tensorflow) (39.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (2.6.11)\n",
            "Requirement already satisfied: mock>=2.0.0 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow) (2.0.0)\n",
            "Requirement already satisfied: h5py in c:\\users\\terry\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow) (2.7.0)\n",
            "Requirement already satisfied: pbr>=0.11 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow) (5.1.3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "You are using pip version 19.0.3, however version 19.1 is available.\n",
            "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-probability in c:\\users\\terry\\anaconda3\\lib\\site-packages (0.6.0)\n",
            "Requirement already satisfied: six>=1.10.0 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-probability) (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\terry\\anaconda3\\lib\\site-packages (from tensorflow-probability) (1.16.2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "You are using pip version 19.0.3, however version 19.1 is available.\n",
            "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-Wa66jl2U0Vx",
        "outputId": "4d7918c0-189b-457b-8ac6-c4eed1b8bc7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "from enum import Enum, IntEnum\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "import collections\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "print(np.__version__)\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "#import tensorflow_probability as tfp\n",
        "#import sonnet as snt\n",
        "#print(snt.__version__)\n",
        "np.set_printoptions(precision=2)\n",
        "from PIL import Image, ImageDraw, ImageChops, ImageFont\n",
        "from IPython.display import display\n",
        "\n",
        "# hosted colab versions\n",
        "# np 1.14.6\n",
        "# tf 1.11.0\n",
        "# snt 1.23"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.16.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX18s2KFEcPI",
        "colab_type": "text"
      },
      "source": [
        "# Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Iq7f4LOtU0V4",
        "colab": {}
      },
      "source": [
        "class Element(IntEnum):\n",
        "    \"\"\"\n",
        "    Enum defining data contents of last dimension\n",
        "    \"\"\"\n",
        "    Vertex = 0\n",
        "    X = 1\n",
        "    Y = 2\n",
        "    Triangle = 3\n",
        "    Offset1 = 4\n",
        "    Offset2 = 5\n",
        "    Size = 5  # size of this enum"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grtWh0hkusEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://raw.githubusercontent.com/terryspitz/ipython_notebooks/master/deeper/TriangleStack.py\n",
        "\n",
        "class TriangleStack(object):\n",
        "  Element = collections.namedtuple(\"Element\", [\"vertex\", \"triangle_offsets\"])\n",
        "  # vertex: Optional(List[float, 2])\n",
        "  # triangle_offsets: Optional(List(int, 2))\n",
        "  ARRAY_SHAPE = [150, 6]\n",
        "\n",
        "  def __init__(self, elements):\n",
        "    self._elements = elements\n",
        "    \n",
        "  @property\n",
        "  def elements(self):\n",
        "    return self._elements\n",
        "  \n",
        "  def get_triangles(self, debug=False):\n",
        "    if debug:\n",
        "      print(\"\\nget_triangles\")\n",
        "    vertices = []\n",
        "    triangles = []\n",
        "    for vertex, triangle in self._elements:\n",
        "      if vertex is not None:\n",
        "        vertices.append(vertex)\n",
        "      if triangle is not None:\n",
        "        last_vertex = len(vertices)-1\n",
        "        triangles.append([last_vertex, last_vertex-1-triangle[0], last_vertex-2-triangle[1]])\n",
        "    if debug:\n",
        "      print(vertices, triangles)\n",
        "      \n",
        "    return {'vertices': vertices, 'triangles': triangles}\n",
        "\n",
        "  @classmethod    \n",
        "  def build_stack(cls, nptri, debug=False):\n",
        "    elements = []\n",
        "    vertex_index_to_old_index = []\n",
        "    vertices = nptri['vertices']\n",
        "    triangles = copy.copy(nptri['triangles'])\n",
        "    if debug:\n",
        "      print(\"\\nbuild_stack\")\n",
        "      #print(vertices)\n",
        "      #print(triangles)\n",
        "    def add_vertex(index):\n",
        "      if debug:\n",
        "        print(\"add_vertex: \", index)\n",
        "      elements.append(TriangleStack.Element(list(vertices[index]), None))\n",
        "      vertex_index_to_old_index.append(index)\n",
        "    def add_triangle(j, k):\n",
        "      i = len(vertex_index_to_old_index)-1\n",
        "      triangle_offsets = [i-j-1, i-k-2]\n",
        "      if debug:\n",
        "        print(\"add_triangle: \", (i, j, k), \"->\", triangle_offsets)\n",
        "      if not elements[-1].triangle_offsets:\n",
        "        elements[-1] = TriangleStack.Element(elements[-1].vertex, triangle_offsets)\n",
        "      else:\n",
        "        elements.append(TriangleStack.Element(None, triangle_offsets))\n",
        "    index = -1\n",
        "    while triangles.max()>-1:\n",
        "      if debug:\n",
        "        print(\"index: %d, last element: %s, vertices: %s\" %(index, elements[-1] if elements else \"\", vertex_index_to_old_index))\n",
        "      if debug:\n",
        "        print(\"Add next vertex\")\n",
        "      for i in range(len(vertex_index_to_old_index)-1, -1, -1):\n",
        "        matches = ((triangles==index) + (triangles==vertex_index_to_old_index[i])).sum(axis=1)\n",
        "        assert 0 <= matches.max() <= 2\n",
        "        matching_tris = (matches==2).nonzero()[0]\n",
        "        if matching_tris.size:\n",
        "          # Add index from triangle with 2 other matches\n",
        "          tri_index = np.random.choice(matching_tris)\n",
        "          indices = list(triangles[tri_index])\n",
        "          if debug:\n",
        "            print(\"matches2: \", matching_tris, matches, tri_index, indices)\n",
        "          indices.remove(index)\n",
        "          indices.remove(vertex_index_to_old_index[i])\n",
        "          index = indices[0]\n",
        "          add_vertex(index)\n",
        "          break\n",
        "      else:\n",
        "        matches = (triangles==index).sum(axis=1)\n",
        "        assert 0 <= matches.max() <= 1\n",
        "        matching_tris = (matches==1).nonzero()[0]\n",
        "        if matching_tris.size:\n",
        "          if debug:\n",
        "            print(\"triangle matches 1, pick from: \", matching_tris)\n",
        "          pick_triangles = triangles[matching_tris]\n",
        "          pick_triangles[pick_triangles==index] = -1\n",
        "        else:\n",
        "          pick_triangles = triangles\n",
        "        # Add index with min uses overall\n",
        "        vertex_use_counts = np.bincount(pick_triangles[pick_triangles>=0].flatten())\n",
        "        vertex_use_counts[vertex_use_counts==0] = 999\n",
        "        if debug:\n",
        "          print(\"vertex_use_counts: \", vertex_use_counts, pick_triangles)\n",
        "        indices = (vertex_use_counts==vertex_use_counts.min()).nonzero()[0]\n",
        "        if debug:\n",
        "          print(\"min indices: \", indices)\n",
        "        index = np.random.choice(indices)\n",
        "        add_vertex(index)        \n",
        "\n",
        "      # Add any triangles possible, closest matches first\n",
        "      if debug:\n",
        "        print(\"Add any triangles\")\n",
        "      for i in range(len(vertex_index_to_old_index)-2, -1, -1):\n",
        "        for j in range(i-1, -1, -1):\n",
        "          matching_tris = ((triangles==index) \n",
        "                           + (triangles==vertex_index_to_old_index[i]) \n",
        "                           + (triangles==vertex_index_to_old_index[j])).sum(axis=1)\n",
        "          if matching_tris.max()==3:\n",
        "            tri = (matching_tris==3).nonzero()[0][0]\n",
        "            if debug:\n",
        "              print(\"tris: \", tri, matching_tris)\n",
        "            add_triangle(i, j)\n",
        "            triangles[tri] = -1\n",
        "\n",
        "    if debug:\n",
        "      print(\"Done build_stack\")\n",
        "    return cls(elements)\n",
        "        \n",
        "  @property\n",
        "  def array(self):\n",
        "    element_list = [[1.0 if vertex is not None else 0.0,\n",
        "                    vertex[0] if vertex is not None else 0.0,\n",
        "                    vertex[1] if vertex is not None else 0.0,\n",
        "                    1.0 if offsets is not None else 0.0,\n",
        "                    float(offsets[0]) if offsets is not None else 0.0,\n",
        "                    float(offsets[1]) if offsets is not None else 0.0\n",
        "                   ] for vertex, offsets in self._elements]\n",
        "    if len(element_list)>MAX_ELEMENTS:\n",
        "      print(\"Too many elements: %d, truncating to %d\" % (len(elements_list), self.ARRAY_SHAPE[0]))\n",
        "      element_list = element_list[:self.ARRAY_SHAPE[0]]\n",
        "    else:\n",
        "      element_list += [[0.0]*6] * (self.ARRAY_SHAPE[0]-len(element_list))\n",
        "    nparray = np.array(element_list)\n",
        "    assert list(nparray.shape) == self.ARRAY_SHAPE, \"%s!=%s\" % (nparray.shape, self.ARRAY_SHAPE)\n",
        "    return nparray\n",
        "\n",
        "  @classmethod    \n",
        "  def from_array(cls, nparray):\n",
        "    assert list(nparray.shape) == cls.ARRAY_SHAPE, \"%s!=%s\" % (nparray.shape, cls.ARRAY_SHAPE)\n",
        "    elements = []\n",
        "    for i in range(0, nparray.shape[0]):\n",
        "      is_vertex = nparray[i, 0]>0.5\n",
        "      is_triangle = nparray[i, 3]>0.5\n",
        "      if not is_vertex and not is_triangle:\n",
        "        break\n",
        "      elements.append(TriangleStack.Element(\n",
        "          [nparray[i, 1], nparray[i, 2]] if is_vertex else None,\n",
        "          [max(0, int(nparray[i, 4])), max(0, int(nparray[i, 5]))] if is_triangle else None))\n",
        "    return cls(elements)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hAsGB4bPU0V9",
        "colab": {}
      },
      "source": [
        "def outlinesToScanlines(outlines, max_points_per_scanline, ygrid, glyphCount):\n",
        "  '''\n",
        "  Pure tensorflow function\n",
        "  Input: 2D tensor of max_outline_points * Stroke (DX,DY,PenDown,EndOfGlyph)\n",
        "  Output: glyphCount * y coord * sorted x coords for each scanline\n",
        "  '''\n",
        "  glyphNums = tf.cast(outlines[:-1,Stroke.EndOfGlyph]>0.5, tf.int32)\n",
        "  glyphNums = tf.cumsum(glyphNums, axis=-1)\n",
        "  glyphNums = tf.concat((tf.zeros([1], dtype=tf.int32), glyphNums), axis=0)\n",
        "  glyphNums2D = tf.expand_dims(tf.range(glyphCount),-1)\n",
        "  glyphNums2D = tf.tile(glyphNums2D, [1,glyphNums.shape[0]])\n",
        "  glyphNums2D = tf.math.equal(glyphNums2D, glyphNums)\n",
        "  glyphNums2D = tf.cast(glyphNums2D, tf.float32)\n",
        "  xs = tf.cumsum(outlines[:,Stroke.DX] * glyphNums2D, axis=1)+1e-2\n",
        "  ys = tf.cumsum(outlines[:,Stroke.DY] * glyphNums2D, axis=1)+1e-2\n",
        "  #offset to create line start x1,y1 to end x2,y2\n",
        "  #add 3rd dimension (size=1) so following interpolation in y is broadcast across all lines\n",
        "  x1s = tf.expand_dims(xs[:,:-1], -1)\n",
        "  x2s = tf.expand_dims(xs[:,1:], -1)\n",
        "  y1s = tf.expand_dims(ys[:,:-1], -1)\n",
        "  y2s = tf.expand_dims(ys[:,1:], -1)\n",
        "  #linear interpolate the x coords for all lines at all y coord\n",
        "  xxs = x1s + (x2s-x1s)*(ygrid-y1s)/(y2s-y1s)\n",
        "  #zero x coords outside of line y bounds or where dy=0 and for hidden lines\n",
        "  in_range = tf.logical_or(tf.logical_and(y1s<ygrid, ygrid<=y2s), tf.logical_and(y2s<ygrid, ygrid<=y1s))\n",
        "  in_range = tf.logical_and(in_range, y2s!=y1s)\n",
        "  visible = outlines[1:,Stroke.PenDown]>0.5\n",
        "  in_range = tf.logical_and(in_range, tf.expand_dims(visible,-1))\n",
        "  xxs = tf.where(in_range, xxs, tf.zeros_like(xxs)) \n",
        "  return tf.nn.top_k(tf.transpose(xxs, perm=(0,2,1)), max_points_per_scanline).values\n",
        "\n",
        "def drawScanLines(xxs, ygrid, cellsize):\n",
        "  '''Draw a glyph rasterisation based on a input y coord array with array of x-intercepts of lines with the y coord'''\n",
        "  columns = int(math.ceil(1000/cellsize))\n",
        "  rows = 1 #math.ceil(xxs.shape[0]/columns)\n",
        "  scale = cellsize * 0.9\n",
        "  im = Image.new('1', size=(columns*cellsize, rows*cellsize), color=(0)) \n",
        "  draw = ImageDraw.Draw(im) \n",
        "  for i in range(xxs.shape[0]):\n",
        "    yy,xx = divmod(i, columns)\n",
        "    xx *= cellsize\n",
        "    yy = (yy+1)*cellsize-1\n",
        "    for xs, y in zip(xxs[i], ygrid):\n",
        "      for j in range(0, xs.shape[0], 2):\n",
        "        if(xs[j]>1e-5):\n",
        "          if(xs[j+1]>1e-5):\n",
        "            draw.line((xx+xs[j]*scale, yy-y*scale, xx+xs[j+1]*scale, yy-y*scale), fill=1)\n",
        "          else:\n",
        "            draw.ellipse((xx+xs[j]*scale-1, yy-y*scale-1, xx+xs[j]*scale+1, yy-y*scale+1), fill=1)\n",
        "  return im  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvlNQrTWwPuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def drawTris(nptris):\n",
        "  shape = nptris.shape\n",
        "  plt.figure(figsize=(shape[1],shape[0]))\n",
        "  ax = plt.plot()\n",
        "  for f in range(shape[0]):\n",
        "    for g in range(shape[1]):\n",
        "      if nptris[f,g]:\n",
        "        vertices = np.array(nptris[f,g][\"vertices\"]) + np.array([g,f])\n",
        "        triangles = np.array(nptris[f,g][\"triangles\"])\n",
        "        tri = matplotlib.tri.Triangulation(vertices[:,0], vertices[:,1], triangles)\n",
        "        plt.tripcolor(tri, range(len(triangles)), cmap=plt.summer())\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dp3yszJeU0WA",
        "colab": {}
      },
      "source": [
        "def parse_proto(example_proto):\n",
        "  features = {\n",
        "    'font_num':  tf.FixedLenFeature([], tf.int64),\n",
        "    'font':      tf.FixedLenFeature([], tf.string),\n",
        "    'glyph_num1': tf.FixedLenFeature([], tf.int64),\n",
        "    'glyph1':     tf.FixedLenFeature([], tf.string),\n",
        "    'tri_stack1': tf.FixedLenFeature(TriangleStack.ARRAY_SHAPE, tf.float32),\n",
        "    'glyph_num2': tf.FixedLenFeature([], tf.int64),\n",
        "    'glyph2':     tf.FixedLenFeature([], tf.string),\n",
        "    'tri_stack2': tf.FixedLenFeature(TriangleStack.ARRAY_SHAPE, tf.float32),\n",
        "  }\n",
        "  data = tf.parse_single_example(example_proto, features)\n",
        "  return data\n",
        "\n",
        "#filename = r'C:\\src\\notebooks\\deeper\\fonts\\fonts26sans.tfrecords'\n",
        "#filename = '/content/gdrive/My Drive/Colab Notebooks/fonts8.tfrecords'\n",
        "#filename = '/home/jupyter/ipython_notebooks/deeper/fonts/fonts8.tfrecords'\n",
        "filename = r'C:\\src\\notebooks\\deeper\\fonts\\fonts26sans_pairs_train.tfrecords'\n",
        "buffer_size=10000\n",
        "batch_size=10\n",
        "dataset = tf.data.TFRecordDataset(filename).map(parse_proto)\n",
        "dataset = dataset.repeat()\n",
        "dataset = dataset.batch(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SYCERW9_ZBfZ",
        "outputId": "4b9359ed-b558-42fd-f46f-a4e4cee9a2ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "# Plot first row of original data\n",
        "dataset_iter = dataset.make_one_shot_iterator()\n",
        "batch = dataset_iter.get_next()\n",
        "with tf.Session():\n",
        "  data = dict(batch)\n",
        "  print(data)\n",
        "  tri_stacks1 = data['tri_stack1'].eval()\n",
        "  tri_stacks2 = data['tri_stack2'].eval()\n",
        "  triangles = [TriangleStack.from_array(t).get_triangles() for t in tri_stacks1]\n",
        "  drawTris(np.expand_dims(np.array(triangles), 0))\n",
        "  triangles = [TriangleStack.from_array(t).get_triangles() for t in tri_stacks2]\n",
        "  drawTris(np.expand_dims(np.array(triangles), 0))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'font': <tf.Tensor 'IteratorGetNext:0' shape=(?,) dtype=string>, 'font_num': <tf.Tensor 'IteratorGetNext:1' shape=(?,) dtype=int64>, 'glyph1': <tf.Tensor 'IteratorGetNext:2' shape=(?,) dtype=string>, 'glyph2': <tf.Tensor 'IteratorGetNext:3' shape=(?,) dtype=string>, 'glyph_num1': <tf.Tensor 'IteratorGetNext:4' shape=(?,) dtype=int64>, 'glyph_num2': <tf.Tensor 'IteratorGetNext:5' shape=(?,) dtype=int64>, 'tri_stack1': <tf.Tensor 'IteratorGetNext:6' shape=(?, 150, 6) dtype=float32>, 'tri_stack2': <tf.Tensor 'IteratorGetNext:7' shape=(?, 150, 6) dtype=float32>}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<matplotlib.figure.Figure at 0x28ee1065b00>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAABZCAYAAAD8bGrjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n"
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<matplotlib.figure.Figure at 0x28ee31b1710>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAABZCAYAAAD8bGrjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAGdRJREFUeJztnX+4HFV5x7/v7t67NwkJiXBVSAI3\nGhCjYpFrBG1LNFhAMcijKFhSaEWmRYQq/gg8VhrkIbRaqdVoN4IWgwqGAk/koaAiTautSAKKhh+S\ngkACPFxMJCGae9nd0z/OnjtnZ2d2Z+/O7pnZ+/08zzz7487dPTtn5sx3vu877xGlFAghhBBCSOfk\nXDeAEEIIIaRfoLAihBBCCEkICitCCCGEkISgsCKEEEIISQgKK0IIIYSQhKCwIoQQQghJCAorQggh\nhJCEoLAihBBCCEkICitCCCGEkIQouPriAw88UI2MjLj6ekIIIYSQ2GzZsuU5pdRwq/WcCauRkRFs\n3rzZ1dcTQgghhMRGRB6Psx5DgSQleK4bQAghhHSMM8eKkCB7JqLF1ezBUg9bQgghhEwNOlYkNcwe\ndN0CQgghpDPoWBFCyBT4s/W+wzpe0Y/7yrXXZaBc1c9/dR7dVkKmE30trB7d5WHXvtbrHX1Q+wPf\nhq0e5s2I/vvxr+jeYHrh7Z3nI+UFKBaAQs2zLOb1YyEHrPpjngiSYuFVnfVVMa/7aaDWT/Y+V8wD\nt5/Jvuolp23wJsVTsaAFVCsO+5LeB8bLwBMfZX9NnXaOpc62s6wO/65lI8BdZ3W3D7ftTD7fdPFL\ngu9wP+wmfR8KnDeU/Gdu2Np8x++mqAKAckUv7ZAXfzEnayOq7Me8AJ/7iYerfuommbzfwoG7x+Od\nfG2KeWuxLn1mDNSvAwDv2+DhfRuY+N9tzrzJw2m17TwUcjmal/D/q1TrXx/SodCe3rgVA8tGevM9\nj+xM5nMWv8RfSG/pa8fKMG8IsZyrdtn1BzS4Vt0WVYAOOxTz9eKqkG9cLzjYF0JktBFT5tF8Vl6A\nf7nbwwVv4pVNEtjiqhhx1BUDfWjWG8iFiyq7P1ferE/Y609lfyXJ2bd4kyG9Yt4P+RVyOtRnHg2F\nnN/XhZwvrOx1Xvo53VfPfoJ91S6bHgeOOzTOmh66IcS67VYBwLaAsDospjCigEoPfe9YJU3Qrdr1\nB0cNCWBcLCO2oq6gASAf0ut50YLKiKpCTi9fuYdX2J0yHnAXx8v+AvjulE2YUzVU0OuZvpk50Liu\nEVikc86+RW9LW8Dax1XwQqUccKfCsNeZdQX7Ko2EhQF75VYZbHH1yM5oF4uuVDqZFo4VkIxrteqH\nHo4+qPF941z1wq0yGNcqjHIFKANY+87o9lx8p1eXX2ULKkCLL/P8mvs8fPAoXl13AyOwmrkXx16j\nB3ojqoBGUTXAS6RE8W71UCxoIVSpBkRUFYDlVBnX6id/Fd6Hi76o+69crX2e8v931hUe9l7CYysu\nxx1awqbHvZiuVX9hxNVhFFKpZ9oIq05Z9UM9OEaJs9Ne0/vBsaKaO1PNWLO8hNWbPMwsALMGG12s\nYP5VL5k9COyZ6P33dpNmQrgV//tBvW+d/G29D84caC6oPnaHhy+cwJP1VPFu9R0L4w7aAqtc1cdd\nGbofbn5/82392IUlHPxP+jONqAo6VxRX7REvJJhMONC4Vb0IA9ps2xkuoIyDddJi7jNpZVoJqyRc\nq7C8KleUq0B+iidrALj0uBJKW+pt70IuPPRx3f0ezjwyWwfyPTvCQy1vnJ+t3xHE3CUYFL37dTXx\n38PY3uZrDM9qf7s+MBbeR0uG3fWREcCVKlBW+rm9rYcKflmFVqLK8NRFer0D/jF94b/VmzwMWeOI\nHVo2z83v/9Ab3B87xrXqBlF3A5L2eeu1Hv7zN83XUZdOZX+K6iP3+6aBAYQYGLfKEMyrOvdodx1a\nUcl8TiHn5/DYdxBmlTsfS98AGcy1apdbP1DCwbOBGYX6E/1+g42i6mN3JP/7h2cl+3lRosol9nbL\n52rhV6ktVph81mB8UWVjO1WVmgs2XgYKl/V2W1xyp4dL7vSwepP+3n3WvmnGlFzg+B/IAevv9/Dt\nX+rFNZtizdrWGb3OrQoSTGTPGslvP/f7XRxiCSsROVFEHhaRbSKyKuTvZ4vImIj8vLack3xTk6Hd\n8gtBUWUw4sqlqAL8nI2psnCOXuYMAjMLvmMVtvSSTsoufPMXus92h4QTs+5W2diCqlhoXLpBM3E1\ntjcbg14zzAXGUKFeSAFaXBln55oVU9uPnl9Vwni55oZV4yW8J80ld4b3U1hTqkpvB4HeDnnRgmsg\n17rsTC9oLa6m3kZbFPQ6DGiTdXE1HWl5uhSRPIC1AE4CsATAGSKyJGTVG5RSf1Rbrk64nYkSV1y1\nqg/kWlQlyaxBvZj8nWKh3rnKmnu1Z1w/7p4IF1iu6NS1Wvcuvc/tN9h9EVWPv68n7VwFcRUG/OQP\nGsPiRmAVC77AGuog/A7ofWC8oi+IzFL+TPd/88e/7+Hj3/cwYe2DL1rPq8oXV7ZbJfDLseTMeJDT\ny00PuhFXxx3qb6+knCuGAZPFiNFmrlVy2zxd5+I4Q/JSANuUUo8CgIhcD+AUAA90s2GuMaLK5GQF\nxdinUlSdvFy7S2kq4mdOUT+OV6yr59pjvuC7YacekZ7f24xJt2q8/v0948C7M/IbWrFw//rXL3Yo\n1qbC8CyE5lyN7fVi5VpFhQHHft9py6aOLVDNsWAnrBvnttMbA9SlJchqb4r5JclgbnypKr3YQuqi\nY+vbdfW9HvK5elGVE19ofu/XHt51eH8cWwbXIcAgUYnsWWHZCFrmW7UmfMyIX9usd8QJ8MwH8KT1\nenvtvSDvEZH7ReRGEVmYSOu6SDPXKsypspPe0ySqbNoNCf74ifq7n4ZCQkkunap2w4FrfuxF3k2Y\nJlHVqWsVZCDfuHSH7m9D24noJX93V/hNHMaZSdoZ7LWouvB2Dy9aaQOVwPOqAiYqwMUhY9s5tYT1\nvOh9y4iqvACDeb3c8X9u3Z7mrlVnbXMZBrRhSDA7xBFWYafW4Cn8ewBGlFJHAvghgGtDP0jkXBHZ\nLCKbx8bG2mtpj2gW/tu1L72iqt1cjXt2eHW5U5NV162cKnMyyYJbZfJGdo/rUg17JnzXKk2iqlOu\nu9/DornAornAy/cDDpjhLybnar/B3ty5mnRI0KlbZU0j1Cy/sDdh12T5G6t8RFVhUmAFxdXqZdHH\nycojSxjIa0FlxFRYoeFeEhThnYQE7ZBU2tyqLGOL0qjt2kk4ULtV6Rvf4xwa2wHYDtQCAE/ZKyil\nfquUMsGXrwE4OuyDlFLrlFKjSqnR4eHhqbQ3UYKu1ei65pM2l05OXwcGaeVaPfxbDw+MeXU5VMHE\ndLuG1QmvTP9vjkrG3TMB7NjT48bEJAnXakZBV2U3LpUtDrpH/f4QJq5aJbGHhQFdiqogdl6hEVYm\noX3N8vQfD2FMlOvnLawqX1y9WAUuf1vr3/XuI0qTocAgWcvBzCpZd62mLlqzlf8W5/rrHgCHicgi\nADsAnA7gA/YKInKQUurp2ssVAB5MtJU9YHSd33F2XlXpZMC7NRuiyuSBVBTw98uAZ17w6v5WUfUi\nqlKbbzAvuh5WRdWf8Jcvcv+b2ykWahLWAT937MNvdP8bkuTQucDjv/NfR1VcP6+Hvzsq36pdXF55\nTtauCl6Y2OUREipt0ktst8qUd5gURrXHzx8ff7sHJ6CuOt4mwZpW0fk28YqFBk/8aQkDhtGO0+My\nny9IMvlW6XWrgBiOlVKqDOB8AHdAC6bvKqW2ishlIrKittoFIrJVRH4B4AIAZ3erwUkzb8gXVcHw\nyXdP049XHt/jRnVAWEjQvBfmThkWHwAsGQaOejlwzIJ0iKo4nHWLh93jqFv2jAM7dqdfVLXrWt33\njN5PD52rxYwRjwM5v2Bob8piNG7XoHPVTumFtLpVtmNVyHXbCewOk2E/SwAZgVWptn/jgx0GzOd6\nkdfXPr2ob+WSbTv9ZPYszBMYR5z22x2ZsTIGlFK3Abgt8N5nrOcXA7g42aa54wcr61/riu0e5g2l\n+0QdRlBo2Za9mZZj8QH++34OidnR0/ubz6pNkrtnoj7Rffc48OV3pLfdU2FsrzeZP/XMCzWnqgAM\nF/zJnHeP6xOpq+rYcZyrYBjQiCrXV56FCGFgD5AupndKgihXqdJBODoY+suim2cwJ/Us5Vb9xyP1\nr21xlYVwYaeuVdrFcwZTMbuDcauCoiqLGDHVLKE9nwOGZ9ZPvmxCBPVX5snMt9UJYXcHnnpD4xXO\nmuXAFf+dLVHVfA5Bb3IdAJhbBH43rhPVxyvWDAC1o3gOgBWvcvvbbXEVt/RCWklKSI1XWl+N6zpR\nyW6rczbWf68RWMGK6u0wozb5d7m2T5ohxqVhFTbFTXhIsL2xLM1hwGYYkZUmgXXXWSW89dr6Poov\nrhr7FnB/MdaMaS+s3r7eT2JvJaqy5Fp9+kfA1S2qQ5twjRFVpgigzdDlAOBh36fT87uNqLJzqta+\nQz+ev9RBgzpkvAKsvLnx5LvicOC01+jnxZozNaPgh2/mzdDiaiCnnareUkKnCaVpCgEC7hypbn2v\nXQjUTjg37lJeWo8RNg+MeZhRAP5Q1hdk1Soa7w9PEe3UN8qyWxVF2kOEQVzXdkuSjJrbybJrX3yn\nate+bMSC41jztpsQFFVDlxtRpU/q+1/p/ne/+sseln7Nw57xelH1zVPr13voOfdt7ZQVh+vHDVuB\njQ/V11EasO78M07rCa8sOXerDFFlGKKKgqbhyjM4R6a9FPPArAG9ZBU7r2pyaVMUmf2vmNdiPlcr\nFNqJA+aaqNyeNLtVWciriku7gjYLbhUwzR2r0XW9qffTa9oZMOcNlVBRXoOoAnwXKx00DoBBQZVV\nTI4U4AuqIBsfAk5YrJ/brhUAvM3ZjQbRrpUJCUaFA9PmVtnY+3y3939T2b3bhI0JcXOstu/WfZwT\n3zkFanl+Of36TQvcnujCwoFAmGvVGA7MmltlRNUjH0m3uAgSFg4EWoUEs3mRPK2FFaBDKTs/ZXbQ\n6E7UCey1/8lQSDAOOrfDw9Dl/t0+YYP9/ld6eH5VOn53K1H10HMejjgwHW1txfo2BOKcok5SdzWf\nXjtETntjiaq0XHlG1XKzKRaAb/3Sw5+/Lpk2V6rdLTjaKjy8dD5w1U/1mHfsAv3arm1l6l4N5nVY\n0S4iDOj3qir9E5unccqTqdIvTlUUzcKBaU9Yt5m2wmp0nX6sLwgaP28kreJqqnfnzLqiuUPlugDg\n/C/4z2853V07kma8DKy8GW3lsJkyC+mg+TGjxZWXaocqyORsBLn610lS6XlOXPh3H7sgfJ2cABX4\n40kh5+dsDeR9x/TIl6VnDIxyraKQ1V6oW5XWMKDtVvUb7dwlmJaLsWakJtCTRbKSbxUHU6cnKs8k\nLeyeAA6eHe/ASnuulQmpJD13YNqwc67SKrDOPbo0mcdmXFsgTaHwqRPMrQrjmAXRuVJ24rs9Frz2\npek/wRnq3Y50jwtR9IOoaiZaG0VufeHXLNEHw0b7hLtVhuwMFkE6qSUTNUeaPSmzq3nS5qzxn+/4\nWHb7J4x+F1WGJcONoirNV552sdWC6MUksN/yUGcnZiNuun08fec9JUxU/AKhwWXTb4Arf6xFVWg7\nQ8aTQ/YvYdHcEg4/IL19F0Xw5Jyl3KpHfuu6Bb1h2UjjDQVZE1XANA4FNpsTsB9Cgu1y0GztoIxX\nmldv7zVGVO0ZB3Zf7G/ng2eX8NSe1n2U1lwrs637g85LL6SBs15fwg2/avwdSd7E4TIEGP39/vFh\nqvgD2bwLslU40M+3yt7dgCTdF2M2086xGl3XSlRFE5y02ZCGkGAnbtXr/1W3v1gAZtauyqMcLFfE\nnS+QpJPgVWcWBsiwGzkKOWDWIPCDR9s/5l2Iqhcr9UtV+Yt5byrkL/OQv8yDrPb6bjqStNFvblUr\n8aqdRL1PZdGtAqahsIpP+gf+pLCToYPFQoNzpvW0XcatihBVWc216i+3ytAfx8v7X1sKzbGyQ+OG\n/2ojUdqVU7XxjMZ+setYVRTw9vXtHR+Dn/UmxwWTl5UlcZXVk/V0JwsXY4ZpJazad6vid6RL16oT\nt+rE63S75xTrb6fOB1yqXiewtxJVWab/RFVzNmwFnn3BdSvax3ZqbbEF+AVaW4mr3eNeqKhyka9Y\nqSI056qqgGX/Fm/8mrPGw1BBl2DIiQ4bFvNIVcXsOCfg1Zsac6zSGAbsN7fKECeJPcsCeNoIq6mH\nANMtrqYqqk653sMp19dCgLWTRLHg3xlon0zMyeWxC3s78MQRVVlzrexioP1HvL5I+5Xne5fo9kWV\nXbCd26ECsOVpD1uejt6/ylV3OYqAdq2MoDJUrJCgca/efE3zY+Rln/cwY0Bvj5xocZVFOpn8l3SX\nfhBVwDROXp8OrLzZCx3QK1XfgSoWAJRDBFoVyOe1u5LPAVvP693JcM6a7jhVrhPZ+zME2JwNW123\nYGq8d0kJt/7aq3OpgPqpn4K5V1vHGi9UhvL1TperPMXbzyxNutN24dDJkGC1Vuzzax4mKsAv/to/\nTl7xRf1/Zk7KvADI6f8ZzNffVJJ2KKrSS7O7NNN+MRZkWjhWnSSsa/xOjUpgN/TStWrmVhlBFZZ4\nng/kith5VObv5nkx31tRBbQvquK6Vq6ZHqIqui+efSFbA+TJh9fPpRkUUs1EkvlbWfn5VeUqsK/m\nWLqoD3f7maVQURUkL8DrvuphyVoPh3/J08VCa+uZnCrT9rSKqrD9LExUmZM5w4C9x97mQVGVdQE8\nLYRVZ6LKkO6QIOCHHKLCDmE1qoKD+1Chfv17vd4OOLJ6av+X9pDg7vHW6/Qbtls19vv0FgdtxkmL\nSzhpcaku17DZ9Dd58Y+/SUGl6pPXZw6U6tbvJXf+hf5uOzRo3CrzfLJtgd9pXg/UHLm0iiqSLVrV\nE8vSxZih70OBphhov2EGxU5yNwo5oDCoQ1TFWtjPfO7PPtT7ndkuBEr6A1tMnffG7A2QhreO6Lb/\nz5NamLcb0isrYHiG/fv92l+uxNWffEN//6SoinDAgxXZn74oG/1o17TKmgPS725VvyNKdXBLWQeM\njo6qzZs3O/luQkj3WL1Jn8yGZ9a/n2VhFcbPdvj13wA/twrwnd+Rudn4zcdc7U3OBQj4eVjmZotf\nfyQbv4OQbiIiW5RSo63W63vHihDSW0xphSyWWGiHpfP7R2z89Jz++S2EuCaWoS0iJ4rIwyKyTURW\nhfy9KCI31P5+t4iMJN1QQkj6ed8GLzSXau07eeImhEwPWgorEckDWAvgJABLAJwhIksCq30QwC6l\n1GIAVwH4h6QbSgjJDllMVCeEkCSI41gtBbBNKfWoUmoCwPUATgmscwqAa2vPbwSwXER6nJJJCHHJ\na75Ct4oQQuLkWM0H8KT1ejuAN0Wto5Qqi8jzAA4A8Jy9koicC+BcADjkkEOm2GRCSJox4oquFSFk\nOhLHsQpznoK3EsZZB0qpdUqpUaXU6PDwcJz2EUIywEs/p++Qe3Zv/ftpLLxICCHdJI6w2g5gofV6\nAYCnotYRkQKA/QHsTKKBhJBsQlFFCJmOxBFW9wA4TEQWicgggNMBbAyssxHAWbXn7wXwI+WqQBYh\npKcYt2psr16CrhUhhEwnWuZY1XKmzgdwB4A8gK8rpbaKyGUANiulNgK4BsB6EdkG7VSd3s1GE0LS\nywNjvZ9fkhBC0kKsAqFKqdsA3BZ47zPW830ATku2aYSQtGPcKoDJ6oQQAkyTSZgJId3h2U+UGiZY\nVpfSrSKETF8orAghHaEuLVFMEUJIDc4VSAhJBIorQgihY0UIIYQQkhjiqiqCiIwBeLwLH30gAhXf\nSWphX2UL9ld2YF9lB/ZVdjhUKdWyurkzYdUtRGSzUmrUdTtIa9hX2YL9lR3YV9mBfdV/MBRICCGE\nEJIQFFaEEEIIIQnRj8JqnesGkNiwr7IF+ys7sK+yA/uqz+i7HCtCCCGEEFf0o2NFCCGEEOKEvhFW\nInKiiDwsIttEZJXr9pBoRGShiNwlIg+KyFYRudB1m0hzRCQvIveJyK2u20KaIyJzReRGEXmodowd\n67pNJBwR+WhtDPyViHxHRIZct4l0Tl8IKxHJA1gL4CQASwCcISJL3LaKNKEM4CKl1KsBHAPgw+yv\n1HMhgAddN4LE4osAbldKHQHg9WC/pRIRmQ/gAgCjSqnXAsgDON1tq0gS9IWwArAUwDal1KNKqQkA\n1wM4xXGbSARKqaeVUvfWnu+BHvjnu20ViUJEFgB4J4CrXbeFNEdE5gD4UwDXAIBSakIp9Tu3rSJN\nKACYISIFADMBPOW4PSQB+kVYzQfwpPV6O3iizgQiMgLgKAB3u20JacI/A/gkgKrrhpCWvALAGIBv\n1EK3V4vILNeNIo0opXYA+DyAJwA8DeB5pdT33baKJEG/CCsJeY+3O6YcEdkPwL8D+Ful1G7X7SGN\niMjJAJ5VSm1x3RYSiwKANwD4qlLqKAB7ATDnNIWIyDzoyMoiAAcDmCUiZ7ptFUmCfhFW2wEstF4v\nAC3VVCMiA9Ci6ltKqZtct4dE8hYAK0TkN9Ah9reJyHVum0SasB3AdqWUcYBvhBZaJH0cD+AxpdSY\nUupFADcBeLPjNpEE6BdhdQ+Aw0RkkYgMQicAbnTcJhKBiAh0DsiDSqkvuG4PiUYpdbFSaoFSagT6\nuPqRUopX1SlFKfUMgCdF5FW1t5YDeMBhk0g0TwA4RkRm1sbE5eCNBn1BwXUDkkApVRaR8wHcAX1n\nxdeVUlsdN4tE8xYAKwH8UkR+XnvvEqXUbQ7bREi/8BEA36pdZD4K4C8dt4eEoJS6W0RuBHAv9J3S\n94FV2PsCVl4nhBBCCEmIfgkFEkIIIYQ4h8KKEEIIISQhKKwIIYQQQhKCwooQQgghJCEorAghhBBC\nEoLCihBCCCEkISisCCGEEEISgsKKEEIIISQh/h8QklkBFVgmwAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKs7lONzEhsk",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vu10ntDkU0WE",
        "outputId": "c5178e53-1b4b-46d1-bb17-d547760c31f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        }
      },
      "source": [
        "def encoder(x, latent_dim, hidden_size):\n",
        "  \"\"\"Construct an inference network parametrizing a Gaussian.\n",
        "  Args:\n",
        "    x: Outlines.\n",
        "    latent_dim: The latent dimensionality.\n",
        "    hidden_size: The size of the neural net hidden layers.\n",
        "  Returns:\n",
        "    mu: Mean parameters for the variational family Normal\n",
        "    sigma: Standard deviation parameters for the variational family Normal\n",
        "  \"\"\"\n",
        "  layers = [x]\n",
        "  with tf.name_scope(\"encoder\"):\n",
        "    # TODO: feed in glyph_onehot\n",
        "    layers.append(tf.keras.layers.Conv1D(filters=4, kernel_size=5)(layers[-1]))\n",
        "    layers.append(tf.keras.layers.MaxPool1D()(layers[-1]))\n",
        "    layers.append(tf.keras.layers.Conv1D(filters=8, kernel_size=5)(layers[-1]))\n",
        "    layers.append(tf.keras.layers.MaxPool1D()(layers[-1]))\n",
        "    layers.append(tf.keras.layers.Conv1D(filters=16, kernel_size=5)(layers[-1]))\n",
        "    layers.append(tf.keras.layers.MaxPool1D()(layers[-1]))\n",
        "    layers.append(tf.keras.layers.Flatten()(layers[-1]))\n",
        "    layers.append(tf.keras.layers.Dense(units=hidden_size, activation='elu')(layers[-1]))\n",
        "    layers.append(tf.keras.layers.Dense(units=hidden_size, activation='elu')(layers[-1]))\n",
        "    # The mean and log variance are unconstrained\n",
        "    latent_mean = tf.keras.layers.Dense(latent_dim)(layers[-1])\n",
        "    latent_log_var = tf.keras.layers.Dense(latent_dim)(layers[-1])\n",
        "    return latent_mean, latent_log_var\n",
        "\n",
        "\n",
        "def decoder(z, rnn_input, hidden_size):\n",
        "  \"\"\"Build a generative network parametrizing the likelihood of the data\n",
        "  Args:\n",
        "    z: Samples of latent variables\n",
        "    hidden_size: Size of the hidden state of the neural net\n",
        "  Returns:\n",
        "    outputs: (batch_size, ARRAY_SHAPE) Generated glyphs\n",
        "    samples: (batch_size, DIST_VARS) Samples used to initialise initial state LSTM\n",
        "    distributions: (batch_size, 6) Prob distributions for LSTM initial state \n",
        "  \"\"\"\n",
        "  layers = [z]\n",
        "  print(\"z:\", layers[-1].shape)\n",
        "  with tf.name_scope(\"decoder\"):\n",
        "    #DIST_VARS = 8\n",
        "    layers.append(tf.keras.layers.Dense(units=hidden_size, activation='elu')(layers[-1]))\n",
        "    #layers.append(tf.keras.layers.Dense(units=hidden_size//DIST_VARS*DIST_VARS, activation='elu')(layers[-1]))\n",
        "    #dist_vars = tf.keras.layers.Dense(units=DIST_VARS, activation='sigmoid')(layers[-1])\n",
        "    #print(\"dist_vars\", dist_vars.shape)\n",
        "    #tf.keras.layers.Reshape([DIST_VARS], name='dist_vars')(layers[-1])\n",
        "    #     element_distributions = [\n",
        "    #         tf.distributions.Bernoulli(logits=dist_vars[...,0], validate_args=True, allow_nan_stats=False),\n",
        "    #         tf.distributions.Uniform(low=dist_vars[...,1], high=dist_vars[...,2], validate_args=True, allow_nan_stats=False),\n",
        "    #         tf.distributions.Uniform(low=dist_vars[...,3], high=dist_vars[...,4], validate_args=True, allow_nan_stats=False),\n",
        "    #         tf.distributions.Bernoulli(logits=dist_vars[...,5], validate_args=True, allow_nan_stats=False),\n",
        "    #         tf.distributions.Exponential(rate=dist_vars[...,6], validate_args=True, allow_nan_stats=False),\n",
        "    #         tf.distributions.Exponential(rate=dist_vars[...,7], validate_args=True, allow_nan_stats=False),\n",
        "    #     ]\n",
        "    #initial_state = tf.stack([tf.cast(dist.sample([6]), tf.float32) for dist in element_distributions])\n",
        "    #initial_state = tf.transpose(initial_state)  # move batch dim from last to first\n",
        "    #initial_state = tf.unstack(initial_state, num=6)#FLAGS.batch_size)\n",
        "    LSTM_UNITS = 32\n",
        "    initial_state = [tf.keras.layers.Dense(units=LSTM_UNITS, activation='sigmoid')(layers[-1]),\n",
        "                    tf.keras.layers.Dense(units=LSTM_UNITS, activation='sigmoid')(layers[-1])]\n",
        "    #print(\"initial_state\", initial_state.shape)\n",
        "    # Note that this cell is not optimized for performance on GPU. Please use\n",
        "    # `tf.keras.layers.CuDNNLSTM` for better performance on GPU.\n",
        "    #     bias_initializer = tf.keras.initializers.Constant()\n",
        "    #     kernel_initializer = tf.keras.initializers.Constant()\n",
        "    #     recurrent_initializer = \n",
        "    lstm_layer = tf.keras.layers.LSTM(units=LSTM_UNITS, return_sequences=True)\n",
        "    layers.append(lstm_layer(rnn_input, initial_state=initial_state))\n",
        "    layers.append(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(TriangleStack.ARRAY_SHAPE[-1]))(layers[-1]))\n",
        "    print(\"posterior: \", layers[-1].shape)\n",
        "    return layers[-1], initial_state\n",
        "\n",
        "    #old:\n",
        "    #output = snt.Linear(np.prod(posterior_sample_shape))(hidden2)\n",
        "    #logits = snt.BatchReshape(posterior_sample_shape)(output) # Q: are these really logits?\n",
        "    #     samples = []\n",
        "    #     samples.append(tf.to_float(\n",
        "    #         tf.distributions.Normal(loc=logits[...,0:2], \n",
        "    #                                            scale=tf.sqrt(tf.clip_by_value(tf.exp(logits[...,2:4]), 1e-2, 2.0)),\n",
        "    #                                            validate_args=True, allow_nan_stats=False)\n",
        "    #     p_x_given_z2 = tf.distributions.Bernoulli(logits=logits[...,4:6],\n",
        "    #                                               validate_args=True, allow_nan_stats=False)\n",
        "    #     return tf.concat([p_x_given_z1.sample(), tf.to_float(p_x_given_z2.sample())], axis=-1), logits, p_x_given_z1, p_x_given_z2\n",
        "\n",
        "\n",
        "# https://github.com/altosaar/variational-autoencoder/blob/master/vae.py\n",
        "\n",
        "def train_graph(data):\n",
        "  tri_stacks1 = data['tri_stack1']\n",
        "  tri_stacks2 = data['tri_stack2']\n",
        "  rnn_input = tri_stacks2[:-1]\n",
        "  rnn_output = tri_stacks2[1:]\n",
        "\n",
        "  with tf.name_scope('variational'):\n",
        "    print(\"input: \", tri_stacks1.shape)\n",
        "    latent_mean, latent_log_var = encoder(x=tri_stacks1,\n",
        "                                          latent_dim=FLAGS.latent_dim,\n",
        "                                          hidden_size=FLAGS.hidden_size)\n",
        "    latent_stdev = tf.sqrt(tf.exp(latent_log_var))\n",
        "    q_z = tf.distributions.Normal(loc=latent_mean, scale=latent_stdev)\n",
        "    assert q_z.reparameterization_type == tf.distributions.FULLY_REPARAMETERIZED\n",
        "\n",
        "  with tf.name_scope('model'):\n",
        "    # The likelihood is Bernoulli-distributed with logits given by the\n",
        "    # generative decoder network\n",
        "    #posterior_predictive_samples, decoder_logits, p_x_given_z1, p_x_given_z2 = decoder(z=q_z.sample(), hidden_size=FLAGS.hidden_size)\n",
        "    outputs, samples = decoder(q_z.sample(), rnn_input, hidden_size=FLAGS.hidden_size)\n",
        "    #tf.summary.image('posterior_predictive', posterior_predictive_samples)\n",
        "\n",
        "    p_z = tf.distributions.Normal(loc=np.zeros(FLAGS.latent_dim, dtype=np.float32),\n",
        "                                  scale=np.ones(FLAGS.latent_dim, dtype=np.float32))\n",
        "    # Take samples from the prior\n",
        "    #     prior_predictive_samples, _, _, _ = decoder(z=p_z.sample(FLAGS.n_samples), hidden_size=FLAGS.hidden_size)\n",
        "    #tf.summary.image('prior_predictive', tf.cast(prior_predictive_samples, tf.float32))\n",
        "\n",
        "    # Take samples from the prior with a placeholder\n",
        "    #z_input = tf.placeholder(tf.float32, [None, FLAGS.latent_dim])\n",
        "    #p_x_given_z_logits = decoder(z=z_input, hidden_size=FLAGS.hidden_size)\n",
        "    #prior_predictive_inp = tf.distributions.Bernoulli(logits=p_x_given_z_logits)\n",
        "    #prior_predictive_inp_sample = prior_predictive_inp.sample()\n",
        "\n",
        "    # Build the evidence lower bound (ELBO) or the negative loss\n",
        "    kl = tf.reduce_sum(tf.distributions.kl_divergence(q_z, p_z), 1)\n",
        "#     expected_log_likelihood1 = tf.reduce_sum(p_x_given_z1.log_prob(x[...,:Stroke.PenDown]), [1, 2])\n",
        "#     expected_log_likelihood2 = tf.reduce_sum(p_x_given_z2.log_prob(x[...,Stroke.PenDown:]), [1, 2])\n",
        "    #global debug1, debug2, debug3\n",
        "    #debug1 = tf.reduce_max(p_x_given_z1.log_prob(x[...,:Stroke.PenDown]))\n",
        "    #debug2 = tf.reduce_max(p_x_given_z2.log_prob(x[...,Stroke.PenDown:]))\n",
        "    #debug3 = kl\n",
        "#     elbo = tf.reduce_sum(expected_log_likelihood1 + expected_log_likelihood2 - kl, 0)\n",
        "    \n",
        "    # try optimising raw encoder/decoder\n",
        "    # compare posterior 0:2 (mean) with DX/DY, and 4:6 (bernoulli p) with bool PenDown/EndOfGlyph\n",
        "    #raw_loss = tf.reduce_sum(kl) + tf.losses.mean_squared_error(x[...,0:2], decoder_logits[...,0:2]) + \\\n",
        "    #  tf.losses.mean_squared_error(x[...,2:4], decoder_logits[...,4:6])\n",
        "    #raw_loss = tf.reduce_sum(kl) + tf.losses.mean_squared_error(x, posterior_predictive_samples)\n",
        "    raw_loss = tf.reduce_sum(kl) + tf.losses.mean_squared_error(rnn_output, outputs)\n",
        "    \n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
        "    #return optimizer.minimize(-elbo), -elbo, posterior_predictive_samples, prior_predictive_samples\n",
        "    return optimizer.minimize(raw_loss), raw_loss#, posterior_predictive_samples, prior_predictive_samples\n",
        "\n",
        "class FLAGS(): \n",
        "  logdir = \"/tmp/tflog\"\n",
        "  iterations = 100000\n",
        "  hidden_size = 32\n",
        "  latent_dim = 16\n",
        "  n_samples = 5\n",
        "  print_every = 100\n",
        "  batch_size = 100\n",
        "\n",
        "# batch_input_shape = [FLAGS.batch_size, max_outline_points, Stroke.Size]\n",
        "# input_shape = [outline_points, Stroke.Size]\n",
        "#posterior_sample_shape = [outline_points, 6] # 2x(mean, stdev, bool)\n",
        "#buffer_size=100\n",
        "#batched_dataset = dataset.shuffle(buffer_size)\n",
        "#batched_dataset = batched_dataset.repeat()\n",
        "#batched_dataset = batched_dataset.batch(FLAGS.batch_size)\n",
        "dataset_iter = dataset.make_one_shot_iterator()\n",
        "batch = dataset_iter.get_next()\n",
        "data = dict(batch)\n",
        "# train_op, loss, posterior_predictive_samples, prior_predictive_samples = train_graph(outlines)\n",
        "train_op, loss = train_graph(data)\n",
        "\n",
        "# Merge all the summaries\n",
        "summary_op = tf.summary.merge_all()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input:  (?, 150, 6)\n",
            "WARNING:tensorflow:From C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-9-b769652341a8>:101: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "WARNING:tensorflow:From C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\distributions\\normal.py:160: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "z: (?, 16)\n",
            "posterior:  (?, 150, 6)\n",
            "WARNING:tensorflow:From <ipython-input-9-b769652341a8>:124: kl_divergence (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "WARNING:tensorflow:From C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FyoU1OxEnlX",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WyejqFKdC_4W",
        "outputId": "c6c8fffb-44e2-4486-c565-0bb6c1c9dad2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2553
        }
      },
      "source": [
        "# Run training\n",
        "with tf.Session() as sess:\n",
        "  init = tf.global_variables_initializer()\n",
        "  sess.run(init)\n",
        "\n",
        "  #print('Saving TensorBoard summaries and images to: %s' % FLAGS.logdir)\n",
        "  #train_writer = tf.summary.FileWriter(FLAGS.logdir, sess.graph)\n",
        "\n",
        "  t0 = time.time()\n",
        "  for i in range(FLAGS.iterations+1):\n",
        "    #print(loss.eval())\n",
        "\n",
        "    #sess.run([tf.add_check_numerics_ops(), train_op])\n",
        "    sess.run(train_op)\n",
        "\n",
        "    # Print progress and save samples every so often\n",
        "    if i % FLAGS.print_every == 0:\n",
        "      np_loss = sess.run(loss)\n",
        "      #train_writer.add_summary(summary_str, i)\n",
        "      print('Iteration: {0:d} loss: {1:.3f} s/iter: {2:.3e}'.format(\n",
        "          i,\n",
        "          np_loss / FLAGS.batch_size  * 1e9,\n",
        "          (time.time() - t0) / FLAGS.print_every))\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Save samples\n",
        "#       np_posterior_samples, np_prior_samples = sess.run(\n",
        "#           [posterior_predictive_samples, prior_predictive_samples])\n",
        "      # for k in range(FLAGS.n_samples):\n",
        "    if i % (FLAGS.print_every*50) == 0:\n",
        "      display(drawOutlines(posterior_predictive_samples.eval(), cellsize=10, glyphCount=glyphNum))\n",
        "      #display(drawOutlines(np_prior_samples, cellsize=30, glyphCount=glyphNum))\n",
        "      #f_name = os.path.join(FLAGS.logdir, 'iter_%d_posterior_predictive_%d_data.jpg' % (i, k))\n",
        "      #imsave(f_name, np_x[k, :, :, 0])\n",
        "      #f_name = os.path.join(FLAGS.logdir, 'iter_%d_posterior_predictive_%d_sample.jpg' % (i, k))\n",
        "      #imsave(f_name, np_posterior_samples[k, :, :, 0])\n",
        "      #f_name = os.path.join(FLAGS.logdir, 'iter_%d_prior_predictive_%d.jpg' % (i, k))\n",
        "      #imsave(f_name, np_prior_samples[k, :, :, 0])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [9,32] vs. [10,32]\n\t [[{{node model/decoder/lstm/while/add_7}}]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-10-a9dd51da0821>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m#sess.run([tf.add_check_numerics_ops(), train_op])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Print progress and save samples every so often\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [9,32] vs. [10,32]\n\t [[node model/decoder/lstm/while/add_7 (defined at <ipython-input-9-b769652341a8>:69) ]]\n\nCaused by op 'model/decoder/lstm/while/add_7', defined at:\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-b769652341a8>\", line 164, in <module>\n    train_op, loss = train_graph(data)\n  File \"<ipython-input-9-b769652341a8>\", line 108, in train_graph\n    outputs, samples = decoder(q_z.sample(), rnn_input, hidden_size=FLAGS.hidden_size)\n  File \"<ipython-input-9-b769652341a8>\", line 69, in decoder\n    layers.append(lstm_layer(rnn_input, initial_state=initial_state))\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\", line 751, in __call__\n    return super(RNN, self).__call__(inputs, **kwargs)\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 554, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\", line 2417, in call\n    inputs, mask=mask, training=training, initial_state=initial_state)\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\", line 823, in call\n    zero_output_for_mask=self.zero_output_for_mask)\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3571, in rnn\n    **while_loop_kwargs)\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 3556, in while_loop\n    return_same_structure)\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 3087, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 3022, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 3525, in <lambda>\n    body = lambda i, lv: (i + 1, orig_body(*lv))\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3556, in _step\n    tuple(states) + tuple(constants))\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\", line 808, in step\n    output, new_states = self.cell.call(inputs, states, **kwargs)\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\", line 2131, in call\n    c, o = self._compute_carry_and_output(x, h_tm1, c_tm1)\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\", line 2063, in _compute_carry_and_output\n    x_o + K.dot(h_tm1_o, self.recurrent_kernel[:, self.units * 3:]))\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 812, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 374, in add\n    \"Add\", x=x, y=y, name=name)\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\terry\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [9,32] vs. [10,32]\n\t [[node model/decoder/lstm/while/add_7 (defined at <ipython-input-9-b769652341a8>:69) ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nEHur-D2U0WU",
        "colab": {}
      },
      "source": [
        "      # Plot the posterior predictive space\n",
        "      #     if FLAGS.latent_dim == 2:\n",
        "      #       np_q_mu = sess.run(q_mu, {x: np_x_fixed})\n",
        "      #       cmap = mpl.colors.ListedColormap(sns.color_palette(\"husl\"))\n",
        "      #       f, ax = plt.subplots(1, figsize=(6 * 1.1618, 6))\n",
        "      #       im = ax.scatter(np_q_mu[:, 0], np_q_mu[:, 1], c=np.argmax(np_y, 1), cmap=cmap,\n",
        "      #                       alpha=0.7)\n",
        "      #       ax.set_xlabel('First dimension of sampled latent variable $z_1$')\n",
        "      #       ax.set_ylabel('Second dimension of sampled latent variable mean $z_2$')\n",
        "      #       ax.set_xlim([-10., 10.])\n",
        "      #       ax.set_ylim([-10., 10.])\n",
        "      #       f.colorbar(im, ax=ax, label='Digit class')\n",
        "      #       plt.tight_layout()\n",
        "      #       plt.savefig(os.path.join(FLAGS.logdir,\n",
        "      #                                'posterior_predictive_map_frame_%d.png' % i))\n",
        "      #       plt.close()\n",
        "\n",
        "      #       nx = ny = 20\n",
        "      #       x_values = np.linspace(-3, 3, nx)\n",
        "      #       y_values = np.linspace(-3, 3, ny)\n",
        "      #       canvas = np.empty((28 * ny, 28 * nx))\n",
        "      #       for ii, yi in enumerate(x_values):\n",
        "      #         for j, xi in enumerate(y_values):\n",
        "      #           np_z = np.array([[xi, yi]])\n",
        "      #           x_mean = sess.run(prior_predictive_inp_sample, {z_input: np_z})\n",
        "      #           canvas[(nx - ii - 1) * 28:(nx - ii) * 28, j *\n",
        "      #                  28:(j + 1) * 28] = x_mean[0].reshape(28, 28)\n",
        "      #       imsave(os.path.join(FLAGS.logdir,\n",
        "      #                           'prior_predictive_map_frame_%d.png' % i), canvas)\n",
        "      # plt.figure(figsize=(8, 10))\n",
        "      # Xi, Yi = np.meshgrid(x_values, y_values)\n",
        "      # plt.imshow(canvas, origin=\"upper\")\n",
        "      # plt.tight_layout()\n",
        "      # plt.savefig()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}