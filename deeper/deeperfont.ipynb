{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deeperfont.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/terryspitz/ipython_notebooks/blob/master/deeper/deeperfont.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "keoxQa2SSnpU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2219
        },
        "outputId": "2e6201f9-c5c3-4709-a4aa-b5893f41ed6f"
      },
      "cell_type": "code",
      "source": [
        "# Install dependencies for colab.research.google.com.\n",
        "!pip install -I tensorflow==1.11.0\n",
        "!pip install tensorflow-probability-gpu\n",
        "!pip install dm-sonnet\n",
        "#!pip install tfp-nightly\n",
        "\n",
        "# Install dependencies for GCP Deep Learning VM.\n",
        "! pip install dm-sonnet\n",
        "! pip install tensorflow-probability-gpu\n",
        "! pip install wrapt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.11.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/d5/38cd4543401708e64c9ee6afa664b936860f4630dd93a49ab863f9998cd2/tensorflow-1.11.0-cp36-cp36m-manylinux1_x86_64.whl (63.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 63.0MB 521kB/s \n",
            "\u001b[?25hCollecting tensorboard<1.12.0,>=1.11.0 (from tensorflow==1.11.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/2f/4d788919b1feef04624d63ed6ea45a49d1d1c834199ec50716edb5d310f4/tensorboard-1.11.0-py3-none-any.whl (3.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.0MB 8.6MB/s \n",
            "\u001b[?25hCollecting astor>=0.6.0 (from tensorflow==1.11.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\n",
            "Collecting setuptools<=39.1.0 (from tensorflow==1.11.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/10/79282747f9169f21c053c562a0baa21815a8c7879be97abd930dbcf862e8/setuptools-39.1.0-py2.py3-none-any.whl (566kB)\n",
            "\u001b[K    100% |████████████████████████████████| 573kB 16.4MB/s \n",
            "\u001b[?25hCollecting grpcio>=1.8.6 (from tensorflow==1.11.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/4c/0a7c55764ac3013ca7a5e9638ee7b161488c0611afc2be465452987a3ccc/grpcio-1.16.0-cp36-cp36m-manylinux1_x86_64.whl (9.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 9.7MB 1.1MB/s \n",
            "\u001b[?25hCollecting gast>=0.2.0 (from tensorflow==1.11.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/78/ff794fcae2ce8aa6323e789d1f8b3b7765f601e7702726f430e814822b96/gast-0.2.0.tar.gz\n",
            "Collecting keras-applications>=1.0.5 (from tensorflow==1.11.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/c4/2ff40221029f7098d58f8d7fb99b97e8100f3293f9856f0fb5834bef100b/Keras_Applications-1.0.6-py2.py3-none-any.whl (44kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 20.2MB/s \n",
            "\u001b[?25hCollecting protobuf>=3.6.0 (from tensorflow==1.11.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/f9/28787754923612ca9bfdffc588daa05580ed70698add063a5629d1a4209d/protobuf-3.6.1-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.1MB 12.4MB/s \n",
            "\u001b[?25hCollecting termcolor>=1.1.0 (from tensorflow==1.11.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
            "Collecting absl-py>=0.1.6 (from tensorflow==1.11.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/63/f505d2d4c21db849cf80bad517f0065a30be6b006b0a5637f1b95584a305/absl-py-0.6.1.tar.gz (94kB)\n",
            "\u001b[K    100% |████████████████████████████████| 102kB 27.9MB/s \n",
            "\u001b[?25hCollecting six>=1.10.0 (from tensorflow==1.11.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl\n",
            "Collecting wheel>=0.26 (from tensorflow==1.11.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/9b/6aebe9e2636d35d1a93772fa644c828303e1d5d124e8a88f156f42ac4b87/wheel-0.32.2-py2.py3-none-any.whl\n",
            "Collecting numpy>=1.13.3 (from tensorflow==1.11.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/7f/9d804d2348471c67a7d8b5f84f9bc59fd1cefa148986f2b74552f8573555/numpy-1.15.4-cp36-cp36m-manylinux1_x86_64.whl (13.9MB)\n",
            "\u001b[K    100% |████████████████████████████████| 13.9MB 2.4MB/s \n",
            "\u001b[?25hCollecting keras-preprocessing>=1.0.3 (from tensorflow==1.11.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/fc/94/74e0fa783d3fc07e41715973435dd051ca89c550881b3454233c39c73e69/Keras_Preprocessing-1.0.5-py2.py3-none-any.whl\n",
            "Collecting markdown>=2.6.8 (from tensorboard<1.12.0,>=1.11.0->tensorflow==1.11.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/6b/5600647404ba15545ec37d2f7f58844d690baf2f81f3a60b862e48f29287/Markdown-3.0.1-py2.py3-none-any.whl (89kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 26.9MB/s \n",
            "\u001b[?25hCollecting werkzeug>=0.11.10 (from tensorboard<1.12.0,>=1.11.0->tensorflow==1.11.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl (322kB)\n",
            "\u001b[K    100% |████████████████████████████████| 327kB 21.2MB/s \n",
            "\u001b[?25hCollecting h5py (from keras-applications>=1.0.5->tensorflow==1.11.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/cb/726134109e7bd71d98d1fcc717ffe051767aac42ede0e7326fd1787e5d64/h5py-2.8.0-cp36-cp36m-manylinux1_x86_64.whl (2.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.8MB 8.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: gast, termcolor, absl-py\n",
            "  Running setup.py bdist_wheel for gast ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/9a/1f/0e/3cde98113222b853e98fc0a8e9924480a3e25f1b4008cedb4f\n",
            "  Running setup.py bdist_wheel for termcolor ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
            "  Running setup.py bdist_wheel for absl-py ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/18/ea/5e/e36e1b8739e78cd2eba0a08fdc602c2b16a4b263912af8cb64\n",
            "Successfully built gast termcolor absl-py\n",
            "Installing collected packages: markdown, werkzeug, six, grpcio, wheel, setuptools, protobuf, numpy, tensorboard, astor, gast, h5py, keras-applications, termcolor, absl-py, keras-preprocessing, tensorflow\n",
            "Successfully installed absl-py-0.6.1 astor-0.7.1 gast-0.2.0 grpcio-1.16.0 h5py-2.8.0 keras-applications-1.0.6 keras-preprocessing-1.0.5 markdown-3.0.1 numpy-1.15.4 protobuf-3.6.1 setuptools-40.5.0 six-1.11.0 tensorboard-1.12.0 tensorflow-1.12.0rc2 termcolor-1.1.0 werkzeug-0.14.1 wheel-0.32.2\n",
            "Collecting tensorflow-probability-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/8a/82b21bfc7115b8463d3d697aa9add3747c85f447b0b6f903a856b9e78c3f/tensorflow_probability_gpu-0.4.0-py2.py3-none-any.whl (582kB)\n",
            "\u001b[K    100% |████████████████████████████████| 583kB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability-gpu) (1.15.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability-gpu) (1.11.0)\n",
            "Collecting tensorflow-gpu>=1.10.0 (from tensorflow-probability-gpu)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/52/01438b81806765936eee690709edc2a975472c4e9d8d465a01840869c691/tensorflow_gpu-1.11.0-cp36-cp36m-manylinux1_x86_64.whl (258.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 258.8MB 86kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x4e74a000 @  0x7fd6153572a4 0x594e17 0x626104 0x51190a 0x4f5277 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x4f3338 0x510fb0 0x5119bd 0x4f6070\n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (1.0.6)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (0.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (1.16.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (0.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (1.0.5)\n",
            "Collecting setuptools<=39.1.0 (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu)\n",
            "  Using cached https://files.pythonhosted.org/packages/8c/10/79282747f9169f21c053c562a0baa21815a8c7879be97abd930dbcf862e8/setuptools-39.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (3.6.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (0.32.2)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (0.6.1)\n",
            "Collecting tensorboard<1.12.0,>=1.11.0 (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu)\n",
            "  Using cached https://files.pythonhosted.org/packages/9b/2f/4d788919b1feef04624d63ed6ea45a49d1d1c834199ec50716edb5d310f4/tensorboard-1.11.0-py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.5->tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (3.0.1)\n",
            "\u001b[31mtensorflow 1.12.0rc2 has requirement tensorboard<1.13.0,>=1.12.0, but you'll have tensorboard 1.11.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: setuptools, tensorboard, tensorflow-gpu, tensorflow-probability-gpu\n",
            "  Found existing installation: setuptools 40.5.0\n",
            "    Uninstalling setuptools-40.5.0:\n",
            "      Successfully uninstalled setuptools-40.5.0\n",
            "  Found existing installation: tensorboard 1.12.0\n",
            "    Uninstalling tensorboard-1.12.0:\n",
            "      Successfully uninstalled tensorboard-1.12.0\n",
            "Successfully installed setuptools-39.1.0 tensorboard-1.11.0 tensorflow-gpu-1.11.0 tensorflow-probability-gpu-0.4.0\n",
            "Collecting dm-sonnet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/fc/863b22ac6fd5630433fbf7ca5f7212db184953b0f3b9c9f3a030d220c986/dm_sonnet-1.26-py3-none-any.whl (629kB)\n",
            "\u001b[K    100% |████████████████████████████████| 634kB 7.7MB/s \n",
            "\u001b[?25hCollecting semantic-version (from dm-sonnet)\n",
            "  Downloading https://files.pythonhosted.org/packages/28/be/3a7241d731ba89063780279a5433f5971c1cf41735b64a9f874b7c3ff995/semantic_version-2.6.0-py3-none-any.whl\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from dm-sonnet) (0.6.1)\n",
            "Collecting contextlib2 (from dm-sonnet)\n",
            "  Downloading https://files.pythonhosted.org/packages/a2/71/8273a7eeed0aff6a854237ab5453bc9aa67deb49df4832801c21f0ff3782/contextlib2-0.5.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from dm-sonnet) (1.11.0)\n",
            "Installing collected packages: semantic-version, contextlib2, dm-sonnet\n",
            "Successfully installed contextlib2-0.5.5 dm-sonnet-1.26 semantic-version-2.6.0\n",
            "Requirement already satisfied: dm-sonnet in /usr/local/lib/python3.6/dist-packages (1.26)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.6/dist-packages (from dm-sonnet) (0.5.5)\n",
            "Requirement already satisfied: semantic-version in /usr/local/lib/python3.6/dist-packages (from dm-sonnet) (2.6.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from dm-sonnet) (0.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from dm-sonnet) (1.11.0)\n",
            "Requirement already satisfied: tensorflow-probability-gpu in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability-gpu) (1.15.4)\n",
            "Requirement already satisfied: tensorflow-gpu>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability-gpu) (1.11.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability-gpu) (1.11.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (0.2.0)\n",
            "Requirement already satisfied: setuptools<=39.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (39.1.0)\n",
            "Requirement already satisfied: tensorboard<1.12.0,>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (1.11.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (1.16.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (0.7.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (1.0.6)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (3.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (0.32.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (1.0.5)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (3.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.5->tensorflow-gpu>=1.10.0->tensorflow-probability-gpu) (2.8.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (1.10.11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-Wa66jl2U0Vx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1628
        },
        "outputId": "dc4e717d-b7bc-4bf8-819b-390365e6cb2c"
      },
      "cell_type": "code",
      "source": [
        "from enum import Enum, IntEnum\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "print(np.__version__)\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import sonnet as snt\n",
        "print(snt.__version__)\n",
        "np.set_printoptions(precision=2)\n",
        "from PIL import Image, ImageDraw, ImageChops, ImageFont\n",
        "from IPython.display import display\n",
        "\n",
        "# hosted colab versions\n",
        "# np 1.14.6\n",
        "# tf 1.11.0\n",
        "# snt 1.23"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0m_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(name, file, filename, details)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[0;34m(name, path, file)\u001b[0m\n\u001b[1;32m    342\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: libcublas.so.9.0: cannot open shared object file: No such file or directory",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a645a8c99876>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msonnet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msnt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Protocol buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[0;32m---> 74\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\n    return _load(spec)\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Iq7f4LOtU0V4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Stroke(IntEnum):\n",
        "    \"\"\"\n",
        "    Enum defining data contents of last dimension\n",
        "    \"\"\"\n",
        "    DX = 0\n",
        "    DY = 1\n",
        "    PenDown = 2  # +1 draw stroke, 0 if this stroke is hidden, i.e. to move between contours in the glyph\n",
        "    EndOfGlyph = 3  # +1 indicates this stroke is the last of the current glyph, else 0\n",
        "    Size = 4  # size of this enum"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DQrXI5ECU0V8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def drawOutlines(outlines, cellsize, glyphCount):\n",
        "  \"\"\"Draw outlines from array.\"\"\"\n",
        "  im = Image.new('1', size=(glyphCount*cellsize, outlines.shape[0]*cellsize), color=(0))\n",
        "  def drawPoints(im, points):\n",
        "    if len(points)<=2:\n",
        "      return im\n",
        "    #temporary image to use to xor each part with main image\n",
        "    im2 = Image.new('1', size=(glyphCount*cellsize, outlines.shape[0]*cellsize), color=(0)) \n",
        "    draw = ImageDraw.Draw(im2).polygon(points, fill=1)\n",
        "    return ImageChops.logical_xor(im, im2)\n",
        "  scale = cellsize * 0.9\n",
        "  for font in range(outlines.shape[0]):\n",
        "    i = 0\n",
        "    for glyphNum in range(glyphCount):\n",
        "      points = []\n",
        "      x,y = (glyphNum * cellsize, (font+1) * cellsize)\n",
        "      #ImageDraw.Draw(im).line((glyphNum * cellsize, font * cellsize, glyphNum * cellsize, (font+1) * cellsize), fill=1)\n",
        "      #ImageDraw.Draw(im).line((glyphNum * cellsize, font * cellsize, (glyphNum+1) * cellsize, font * cellsize), fill=1)\n",
        "      while True:\n",
        "        x += outlines[font,i,Stroke.DX]*scale\n",
        "        y -= outlines[font,i,Stroke.DY]*scale\n",
        "        #print(outlines[font,i], x,y)\n",
        "        if outlines[font,i,Stroke.PenDown]>0.5:\n",
        "          points += (x,y)\n",
        "        else:\n",
        "          im = drawPoints(im, points)\n",
        "          points=[(x,y)]\n",
        "        i+=1\n",
        "        if outlines[font,i-1,Stroke.EndOfGlyph]>0.5 or i>=outlines.shape[1]:\n",
        "          break\n",
        "      im = drawPoints(im, points)\n",
        "      if i>=outlines.shape[1]:\n",
        "        break\n",
        "  return im"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hAsGB4bPU0V9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def outlinesToScanlines(outlines, max_points_per_scanline, ygrid, glyphCount):\n",
        "    '''\n",
        "    Pure tensorflow function\n",
        "    Input: 2D tensor of max_outline_points * Stroke (DX,DY,PenDown,EndOfGlyph)\n",
        "    Output: glyphCount * y coord * sorted x coords for each scanline\n",
        "    '''\n",
        "    glyphNums = tf.cast(outlines[:-1,Stroke.EndOfGlyph]>0.5, tf.int32)\n",
        "    glyphNums = tf.cumsum(glyphNums, axis=-1)\n",
        "    glyphNums = tf.concat((tf.zeros([1], dtype=tf.int32), glyphNums), axis=0)\n",
        "    glyphNums2D = tf.expand_dims(tf.range(glyphCount),-1)\n",
        "    glyphNums2D = tf.tile(glyphNums2D, [1,glyphNums.shape[0]])\n",
        "    glyphNums2D = tf.math.equal(glyphNums2D, glyphNums)\n",
        "    glyphNums2D = tf.cast(glyphNums2D, tf.float32)\n",
        "    xs = tf.cumsum(outlines[:,Stroke.DX] * glyphNums2D, axis=1)+1e-2\n",
        "    ys = tf.cumsum(outlines[:,Stroke.DY] * glyphNums2D, axis=1)+1e-2\n",
        "    #offset to create line start x1,y1 to end x2,y2\n",
        "    #add 3rd dimension (size=1) so following interpolation in y is broadcast across all lines\n",
        "    x1s = tf.expand_dims(xs[:,:-1], -1)\n",
        "    x2s = tf.expand_dims(xs[:,1:], -1)\n",
        "    y1s = tf.expand_dims(ys[:,:-1], -1)\n",
        "    y2s = tf.expand_dims(ys[:,1:], -1)\n",
        "    #linear interpolate the x coords for all lines at all y coord\n",
        "    xxs = x1s + (x2s-x1s)*(ygrid-y1s)/(y2s-y1s)\n",
        "    #zero x coords outside of line y bounds or where dy=0 and for hidden lines\n",
        "    in_range = tf.logical_or(tf.logical_and(y1s<ygrid, ygrid<=y2s), tf.logical_and(y2s<ygrid, ygrid<=y1s))\n",
        "    in_range = tf.logical_and(in_range, y2s!=y1s)\n",
        "    visible = outlines[1:,Stroke.PenDown]>0.5\n",
        "    in_range = tf.logical_and(in_range, tf.expand_dims(visible,-1))\n",
        "    xxs = tf.where(in_range, xxs, tf.zeros_like(xxs)) \n",
        "    return tf.nn.top_k(tf.transpose(xxs, perm=(0,2,1)), max_points_per_scanline).values\n",
        "  \n",
        "def drawScanLines(xxs, ygrid, cellsize):\n",
        "    '''Draw a glyph rasterisation based on a input y coord array with array of x-intercepts of lines with the y coord'''\n",
        "    columns = math.ceil(1000/cellsize)\n",
        "    rows = 1 #math.ceil(xxs.shape[0]/columns)\n",
        "    scale = cellsize * 0.9\n",
        "    im = Image.new('1', size=(columns*cellsize, rows*cellsize), color=(0)) \n",
        "    draw = ImageDraw.Draw(im) \n",
        "    for i in range(xxs.shape[0]):\n",
        "      yy,xx = divmod(i, columns)\n",
        "      xx *= cellsize\n",
        "      yy = (yy+1)*cellsize-1\n",
        "      for xs, y in zip(xxs[i], ygrid):\n",
        "        for j in range(0, xs.shape[0], 2):\n",
        "          if(xs[j]>1e-5):\n",
        "            if(xs[j+1]>1e-5):\n",
        "              draw.line((xx+xs[j]*scale, yy-y*scale, xx+xs[j+1]*scale, yy-y*scale), fill=1)\n",
        "            else:\n",
        "              draw.ellipse((xx+xs[j]*scale-1, yy-y*scale-1, xx+xs[j]*scale+1, yy-y*scale+1), fill=1)\n",
        "    return im  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dp3yszJeU0WA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "96b2763d-5b60-49ad-d358-281c27d2154b"
      },
      "cell_type": "code",
      "source": [
        "def parse_proto(example_proto):\n",
        "  features = {\n",
        "    'glyphNum': tf.FixedLenFeature([1], tf.int64),\n",
        "    'max_outline_points': tf.FixedLenFeature([1], tf.int64),\n",
        "    'outline': tf.FixedLenFeature([5000*4], tf.float32),\n",
        "  }\n",
        "  return tf.parse_single_example(example_proto, features)\n",
        "\n",
        "#filename = '/content/gdrive/My Drive/Colab Notebooks/fonts8.tfrecords'\n",
        "filename = '/home/jupyter/ipython_notebooks/deeper/fonts/fonts8.tfrecords'\n",
        "dataset = tf.data.TFRecordDataset(filename).map(parse_proto)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-752ec9ebe7ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#filename = '/content/gdrive/My Drive/Colab Notebooks/fonts8.tfrecords'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/jupyter/ipython_notebooks/deeper/fonts/fonts8.tfrecords'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_proto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "SYCERW9_ZBfZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "outputId": "4dd2be25-78ae-48d5-f945-05aea3a14857"
      },
      "cell_type": "code",
      "source": [
        "# Plot first row of original data\n",
        "dataset_iter = dataset.make_one_shot_iterator()\n",
        "batch = dataset_iter.get_next()\n",
        "\n",
        "with tf.Session():\n",
        "  glyphNum = int(batch[\"glyphNum\"].eval()[0])\n",
        "  max_outline_points = batch[\"max_outline_points\"].eval()[0]\n",
        "  outlines = tf.reshape(batch[\"outline\"], (-1, max_outline_points, Stroke.Size)).eval()\n",
        "  print(outlines.shape)\n",
        "  assert glyphNum == 63\n",
        "  assert max_outline_points == 5000\n",
        "  assert outlines.shape[0] == 1\n",
        "\n",
        "outlines = outlines[:,0:200,:]\n",
        "print(\"outline shape (fonts, points_per_outline, Stroke.Size): \", outlines.shape)\n",
        "display(drawOutlines(outlines, cellsize=30, glyphCount=glyphNum))\n",
        "\n",
        "with tf.Session():\n",
        "  y_divisions = 30\n",
        "  max_points_per_scanline = 8\n",
        "  ygrid = np.linspace(0.0, 1.0, y_divisions, endpoint=False) #y coordinates to render on\n",
        "  scanlines = outlinesToScanlines(outlines[0], max_points_per_scanline, ygrid, glyphNum)\n",
        "  print(\"scanlines shape (glyphs, lines, max_points_per_line): \", scanlines.shape)\n",
        "  display(drawScanLines(scanlines.eval(), ygrid, 30))\n",
        "  "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 5000, 4)\n",
            "outline shape (fonts, points_per_outline, Stroke.Size):  (1, 200, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAB2IAAAAeAQAAAADWBeOaAAABdUlEQVR4nO3YMW7bMBiA0UdKqDwY\nsLobiY+RAkXjI3X01KhTL9ND8Ag5Art3UDehcMQOiXsDloDQNwmaPkCUqJ/8t3ndivvkPrcuqWuh\nyw4lUWZlad1TURTQw0lnz7vGRTXF20XpR3vRj/ctcyqLYmLAyvh1ZUmNk6oaHjhmdy4eHUPuUuOg\niv6u5BWPfsrrqV1NbdHQcxzBFwulbVBV0Zg5T+AKxmYx1UUrFk4uYSE8lbF1Uz3RCRPX13d4PTfN\nqSy6Ykb3bG7bUl/0QMicj6l1S3296buIIYehdUx10YwEk4ypXUt9MZj1kl+7tzub3m9jZjfh97lt\nyb8QXzee3bZX8E3sEwfM5QShaU11h7J6LIssD4lQrq2LKrrNQBt/pm/6Q7jzIVzAidtosE3R7WAq\n5Csp2vQp3B0uyJGxY9M/y/FsDiT2ye6I3Liopvg2AkWfp5chW1/OjYuqWhy67GNXVp7K1Dqnstnh\nW/JpKFfut/w9hj8OgGP2GkIZawAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=1 size=1890x30 at 0x7F2848DF8DA0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "scanlines shape (glyphs, lines, max_points_per_line):  (63, 30, 8)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/wAAAAeAQAAAAC5vihCAAABT0lEQVR4nO3WMbLTMBSF4c+y58VF\nZuzyFQxJRc0CMokXloEw0NOyHC/h7QAvwVCpSCwKJzT0Nw2nU/X/0ujqiP+RqSddmVQlKyWWnlQN\nTY/eixdcggUSWnyj/b34tR+CBR751Oq/D8yhfGyu7GY72clrNW+mUHxi5H7xTmauoXxJi/26WGSW\nPtZAd+Awey+vA1nNofhGjwFyBT+XKlQg2SXOrX1NDkU/BI5UmeXdW5q5jeEC99RTNHpN4/xBmn8Y\nXOpnCCQZE1ya6RkCFRrc2uEJeJIZI9frHtFdSHqU4d8E/wfUZdGVLFfTZkRZYvlpq/JR81jHPoNI\nvuyV6iK7d1L0CfQXjlTSvYjD6/gxBuiN4YWQdjhDM1HjLVhgsW56frVst9JazYECw3rz024aSz27\nhY9B1tWTQ10WPpdLNJ6s+zo51iVzCh7Bf9JN8cw/zG9ln9bEKW0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=1 size=1020x30 at 0x7F2845DB1EB8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Vu10ntDkU0WE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "dd5fb6d7-df2d-4c0b-f43a-984e95b1f7b4"
      },
      "cell_type": "code",
      "source": [
        "def encoder(x, latent_dim, hidden_size):\n",
        "  \"\"\"Construct an inference network parametrizing a Gaussian.\n",
        "  Args:\n",
        "    x: Outlines.\n",
        "    latent_dim: The latent dimensionality.\n",
        "    hidden_size: The size of the neural net hidden layers.\n",
        "  Returns:\n",
        "    mu: Mean parameters for the variational family Normal\n",
        "    sigma: Standard deviation parameters for the variational family Normal\n",
        "  \"\"\"\n",
        "  with tf.name_scope(\"encoder\"):\n",
        "    flat = snt.BatchFlatten()(x)\n",
        "    hidden1 = tf.nn.elu(snt.Linear(hidden_size)(flat))\n",
        "    hidden2 = tf.nn.elu(snt.Linear(hidden_size)(hidden1))\n",
        "    latent_mean = snt.Linear(latent_dim)(hidden2)  # The mean parameter is unconstrained\n",
        "    latent_log_var = snt.Linear(latent_dim)(hidden2)\n",
        "    return latent_mean, latent_log_var\n",
        "\n",
        "\n",
        "def decoder(z, hidden_size):\n",
        "  \"\"\"Build a generative network parametrizing the likelihood of the data\n",
        "  Args:\n",
        "    z: Samples of latent variables\n",
        "    hidden_size: Size of the hidden state of the neural net\n",
        "  Returns:\n",
        "    gaussian_logits for the 2x real variables\n",
        "    bernoulli_logits for the likelihood of the 2x bool data\n",
        "  \"\"\"\n",
        "  with tf.name_scope(\"decoder\"):\n",
        "    hidden1 = tf.nn.elu(snt.Linear(hidden_size)(z))\n",
        "    hidden2 = tf.nn.elu(snt.Linear(hidden_size)(hidden1))\n",
        "    output = snt.Linear(np.prod(posterior_sample_shape))(hidden2)\n",
        "    logits = snt.BatchReshape(posterior_sample_shape)(output) # Q: are these really logits?\n",
        "    p_x_given_z1 = tf.distributions.Normal(loc=logits[...,0:2], \n",
        "                                           scale=tf.sqrt(tf.clip_by_value(tf.exp(logits[...,2:4]), 1e-2, 2.0)),\n",
        "                                           validate_args=True, allow_nan_stats=False)\n",
        "    p_x_given_z2 = tf.distributions.Bernoulli(logits=logits[...,4:6],\n",
        "                                              validate_args=True, allow_nan_stats=False)\n",
        "    return tf.concat([p_x_given_z1.sample(), tf.to_float(p_x_given_z2.sample())], axis=-1), logits, p_x_given_z1, p_x_given_z2\n",
        "\n",
        "\n",
        "# https://github.com/altosaar/variational-autoencoder/blob/master/vae.py\n",
        "\n",
        "def train_graph(x):\n",
        "  with tf.name_scope('variational'):\n",
        "    latent_mean, latent_log_var = encoder(x=x,\n",
        "                                          latent_dim=FLAGS.latent_dim,\n",
        "                                          hidden_size=FLAGS.hidden_size)\n",
        "    latent_stdev = tf.sqrt(tf.exp(latent_log_var))\n",
        "    q_z = tf.distributions.Normal(loc=latent_mean, scale=latent_stdev)\n",
        "    assert q_z.reparameterization_type == tf.distributions.FULLY_REPARAMETERIZED\n",
        "\n",
        "  with tf.name_scope('model'):\n",
        "    # The likelihood is Bernoulli-distributed with logits given by the\n",
        "    # generative decoder network\n",
        "    posterior_predictive_samples, decoder_logits, p_x_given_z1, p_x_given_z2 = decoder(z=q_z.sample(), hidden_size=FLAGS.hidden_size)\n",
        "    #tf.summary.image('posterior_predictive', posterior_predictive_samples)\n",
        "\n",
        "    # Take samples from the prior\n",
        "    p_z = tf.distributions.Normal(loc=np.zeros(FLAGS.latent_dim, dtype=np.float32),\n",
        "                                  scale=np.ones(FLAGS.latent_dim, dtype=np.float32))\n",
        "    prior_predictive_samples, _, _, _ = decoder(z=p_z.sample(FLAGS.n_samples), hidden_size=FLAGS.hidden_size)\n",
        "    #tf.summary.image('prior_predictive', tf.cast(prior_predictive_samples, tf.float32))\n",
        "\n",
        "    # Take samples from the prior with a placeholder\n",
        "    #z_input = tf.placeholder(tf.float32, [None, FLAGS.latent_dim])\n",
        "    #p_x_given_z_logits = decoder(z=z_input, hidden_size=FLAGS.hidden_size)\n",
        "    #prior_predictive_inp = tf.distributions.Bernoulli(logits=p_x_given_z_logits)\n",
        "    #prior_predictive_inp_sample = prior_predictive_inp.sample()\n",
        "\n",
        "    # Build the evidence lower bound (ELBO) or the negative loss\n",
        "    kl = tf.reduce_sum(tf.distributions.kl_divergence(q_z, p_z), 1)\n",
        "    expected_log_likelihood1 = tf.reduce_sum(p_x_given_z1.log_prob(x[...,:Stroke.PenDown]), [1, 2])\n",
        "    expected_log_likelihood2 = tf.reduce_sum(p_x_given_z2.log_prob(x[...,Stroke.PenDown:]), [1, 2])\n",
        "    #global debug1, debug2, debug3\n",
        "    #debug1 = tf.reduce_max(p_x_given_z1.log_prob(x[...,:Stroke.PenDown]))\n",
        "    #debug2 = tf.reduce_max(p_x_given_z2.log_prob(x[...,Stroke.PenDown:]))\n",
        "    #debug3 = kl\n",
        "    elbo = tf.reduce_sum(expected_log_likelihood1 + expected_log_likelihood2 - kl, 0)\n",
        "    \n",
        "    # try optimising raw encoder/decoder\n",
        "    # compare posterior 0:2 (mean) with DX/DY, and 4:6 (bernoulli p) with bool PenDown/EndOfGlyph\n",
        "    raw_loss = tf.reduce_sum(kl) + tf.losses.mean_squared_error(x[...,0:2], decoder_logits[...,0:2]) + \\\n",
        "      tf.losses.mean_squared_error(x[...,2:4], decoder_logits[...,4:6])\n",
        "    \n",
        "    # Q: do we have any regularisation of encoder/decoder weights?\n",
        "    optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001)\n",
        "    #return optimizer.minimize(-elbo), -elbo, posterior_predictive_samples, prior_predictive_samples\n",
        "    return optimizer.minimize(raw_loss), raw_loss, posterior_predictive_samples, prior_predictive_samples\n",
        "\n",
        "class FLAGS(): \n",
        "  logdir = \"/tmp/tflog\"\n",
        "  iterations = 100000\n",
        "  hidden_size = 100\n",
        "  latent_dim = 64\n",
        "  n_samples = 5\n",
        "  print_every = 100\n",
        "  batch_size = 100\n",
        "\n",
        "batch_input_shape = [FLAGS.batch_size, max_outline_points, Stroke.Size]\n",
        "outline_points = 200\n",
        "input_shape = [outline_points, Stroke.Size]\n",
        "posterior_sample_shape = [outline_points, 6] # 2x(mean, stdev, bool)\n",
        "buffer_size=100\n",
        "batched_dataset = dataset.shuffle(buffer_size)\n",
        "batched_dataset = batched_dataset.repeat()\n",
        "batched_dataset = batched_dataset.batch(FLAGS.batch_size)\n",
        "batch = batched_dataset.make_one_shot_iterator().get_next()\n",
        "outlines = tf.reshape(batch['outline'], batch_input_shape)\n",
        "print(outlines.shape)\n",
        "outlines = outlines[:,0:outline_points,:]\n",
        "print(outlines.shape)\n",
        "train_op, loss, posterior_predictive_samples, prior_predictive_samples = train_graph(outlines)\n",
        "\n",
        "# Merge all the summaries\n",
        "summary_op = tf.summary.merge_all()\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(100, 5000, 4)\n",
            "(100, 200, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WyejqFKdC_4W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Run training\n",
        "with tf.Session() as sess:\n",
        "  init = tf.global_variables_initializer()\n",
        "  sess.run(init)\n",
        "\n",
        "  #print('Saving TensorBoard summaries and images to: %s' % FLAGS.logdir)\n",
        "  #train_writer = tf.summary.FileWriter(FLAGS.logdir, sess.graph)\n",
        "\n",
        "  t0 = time.time()\n",
        "  for i in range(FLAGS.iterations+1):\n",
        "    #print(loss.eval())\n",
        "\n",
        "    #sess.run([tf.add_check_numerics_ops(), train_op])\n",
        "    sess.run(train_op)\n",
        "\n",
        "    # Print progress and save samples every so often\n",
        "    if i % FLAGS.print_every == 0:\n",
        "      np_loss = sess.run(loss)\n",
        "      #train_writer.add_summary(summary_str, i)\n",
        "      print('Iteration: {0:d} loss: {1:.3f} s/iter: {2:.3e}'.format(\n",
        "          i,\n",
        "          np_loss / FLAGS.batch_size  * 1e9,\n",
        "          (time.time() - t0) / FLAGS.print_every))\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Save samples\n",
        "#       np_posterior_samples, np_prior_samples = sess.run(\n",
        "#           [posterior_predictive_samples, prior_predictive_samples])\n",
        "      # for k in range(FLAGS.n_samples):\n",
        "    if i % (FLAGS.print_every*50) == 0:\n",
        "      display(drawOutlines(posterior_predictive_samples.eval(), cellsize=10, glyphCount=glyphNum))\n",
        "      #display(drawOutlines(np_prior_samples, cellsize=30, glyphCount=glyphNum))\n",
        "      #f_name = os.path.join(FLAGS.logdir, 'iter_%d_posterior_predictive_%d_data.jpg' % (i, k))\n",
        "      #imsave(f_name, np_x[k, :, :, 0])\n",
        "      #f_name = os.path.join(FLAGS.logdir, 'iter_%d_posterior_predictive_%d_sample.jpg' % (i, k))\n",
        "      #imsave(f_name, np_posterior_samples[k, :, :, 0])\n",
        "      #f_name = os.path.join(FLAGS.logdir, 'iter_%d_prior_predictive_%d.jpg' % (i, k))\n",
        "      #imsave(f_name, np_prior_samples[k, :, :, 0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nEHur-D2U0WU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "      # Plot the posterior predictive space\n",
        "      #     if FLAGS.latent_dim == 2:\n",
        "      #       np_q_mu = sess.run(q_mu, {x: np_x_fixed})\n",
        "      #       cmap = mpl.colors.ListedColormap(sns.color_palette(\"husl\"))\n",
        "      #       f, ax = plt.subplots(1, figsize=(6 * 1.1618, 6))\n",
        "      #       im = ax.scatter(np_q_mu[:, 0], np_q_mu[:, 1], c=np.argmax(np_y, 1), cmap=cmap,\n",
        "      #                       alpha=0.7)\n",
        "      #       ax.set_xlabel('First dimension of sampled latent variable $z_1$')\n",
        "      #       ax.set_ylabel('Second dimension of sampled latent variable mean $z_2$')\n",
        "      #       ax.set_xlim([-10., 10.])\n",
        "      #       ax.set_ylim([-10., 10.])\n",
        "      #       f.colorbar(im, ax=ax, label='Digit class')\n",
        "      #       plt.tight_layout()\n",
        "      #       plt.savefig(os.path.join(FLAGS.logdir,\n",
        "      #                                'posterior_predictive_map_frame_%d.png' % i))\n",
        "      #       plt.close()\n",
        "\n",
        "      #       nx = ny = 20\n",
        "      #       x_values = np.linspace(-3, 3, nx)\n",
        "      #       y_values = np.linspace(-3, 3, ny)\n",
        "      #       canvas = np.empty((28 * ny, 28 * nx))\n",
        "      #       for ii, yi in enumerate(x_values):\n",
        "      #         for j, xi in enumerate(y_values):\n",
        "      #           np_z = np.array([[xi, yi]])\n",
        "      #           x_mean = sess.run(prior_predictive_inp_sample, {z_input: np_z})\n",
        "      #           canvas[(nx - ii - 1) * 28:(nx - ii) * 28, j *\n",
        "      #                  28:(j + 1) * 28] = x_mean[0].reshape(28, 28)\n",
        "      #       imsave(os.path.join(FLAGS.logdir,\n",
        "      #                           'prior_predictive_map_frame_%d.png' % i), canvas)\n",
        "      # plt.figure(figsize=(8, 10))\n",
        "      # Xi, Yi = np.meshgrid(x_values, y_values)\n",
        "      # plt.imshow(canvas, origin=\"upper\")\n",
        "      # plt.tight_layout()\n",
        "      # plt.savefig()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}